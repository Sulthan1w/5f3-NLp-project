{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import re\n",
    "sd_data=urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "article=sd_data.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_a=bs.BeautifulSoup(article,'lxml')\n",
    "paragraphs=parse_a.find_all('p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=\"\"\n",
    "for i in paragraphs:\n",
    "    texts=texts+i.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
      "Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\n",
      "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it is confronted with.\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]\n",
      "In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[9] parsing,[10][11] and many others. This is increasingly important in medicine and healthcare, where NLP is being used to analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.[12]\n",
      "In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming.\n",
      "More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: \n",
      "Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used:\n",
      "Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\n",
      "Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature (complex-valued embeddings,[17] and neural networks in general have also been proposed, for e.g. speech[18]). Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\n",
      "A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[19] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Latest works tend to use non-technical structure of a given task to build proper neural network.[20]\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n",
      "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n",
      "Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[36]\n",
      "Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n",
      "Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[37] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[38] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[39] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\n",
      "As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[40] with two defining aspects:\n",
      "Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[42] functional grammar,[43] construction grammar,[44] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[45] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[46] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).[47]\n",
      " Media related to Natural language processing at Wikimedia Commons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing square brackets,parenthesis,specialcharacters,digits\n",
    "texts=re.sub(r'[[0-9]*]',' ',texts)\n",
    "texts=re.sub(r's+',' ',texts)\n",
    "formatted_texts=re.sub('[^a-zA-Z]',' ',texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language proce ing (NLP) i  a  ubfield of lingui tic , computer  cience, and artificial intelligence concerned with the interaction  between computer  and human language, in particular how to program computer  to proce  and analyze large amount  of natural language data.  The goal i  a computer capable of \"under tanding\" the content  of document , including the contextual nuance  of the language within them. The technology can then accurately extract information and in ight  contained in the document  a  well a  categorize and organize the document  them elve .\n",
      "Challenge  in natural language proce ing frequently involve  peech recognition, natural language under tanding, and natural language generation.\n",
      "Natural language proce ing ha  it  root  in the 1950 . Already in 1950, Alan Turing publi hed an article titled \"Computing Machinery and Intelligence\" which propo ed what i  now called the Turing te t a  a criterion of intelligence, a ta k that involve  the automated interpretation and generation of natural language, but at the time not articulated a  a problem  eparate from artificial intelligence.\n",
      "The premi e of  ymbolic NLP i  well- ummarized by John Searle'  Chine e room experiment: Given a collection of rule  (e.g., a Chine e phra ebook, with que tion  and matching an wer ), the computer emulate  natural language under tanding (or other NLP ta k ) by applying tho e rule  to the data it i  confronted with.\n",
      "Up to the 1980 , mo t natural language proce ing  y tem  were ba ed on complex  et  of hand-written rule .  Starting in the late 1980 , however, there wa  a revolution in natural language proce ing with the introduction of machine learning algorithm  for language proce ing.  Thi  wa  due to both the  teady increa e in computational power ( ee Moore'  law) and the gradual le ening of the dominance of Chom kyan theorie  of lingui tic  (e.g. tran formational grammar), who e theoretical underpinning  di couraged the  ort of corpu  lingui tic  that underlie  the machine-learning approach to language proce ing. \n",
      "In the 2010 , repre entation learning and deep neural network- tyle machine learning method  became wide pread in natural language proce ing, due in part to a flurry of re ult   howing that  uch technique    can achieve  tate-of-the-art re ult  in many natural language ta k , for example in language modeling,  par ing,   and many other . Thi  i  increa ingly important in medicine and healthcare, where NLP i  being u ed to analyze note  and text in electronic health record  that would otherwi e be inacce ible for  tudy when  eeking to improve care. \n",
      "In the early day , many language-proce ing  y tem  were de igned by  ymbolic method , i.e., the hand-coding of a  et of rule , coupled with a dictionary lookup:    uch a  by writing grammar  or devi ing heuri tic rule  for  temming.\n",
      "More recent  y tem  ba ed on machine-learning algorithm  have many advantage  over hand-produced rule : \n",
      "De pite the popularity of machine learning in NLP re earch,  ymbolic method  are  till (2020) commonly u ed:\n",
      "Since the  o-called \" tati tical revolution\"   in the late 1980  and mid-1990 , much natural language proce ing re earch ha  relied heavily on machine learning. The machine-learning paradigm call  in tead for u ing  tati tical inference to automatically learn  uch rule  through the analy i  of large corpora (the plural form of corpu , i  a  et of document , po ibly with human or computer annotation ) of typical real-world example .\n",
      "Many different cla e  of machine-learning algorithm  have been applied to natural-language-proce ing ta k . The e algorithm  take a  input a large  et of \"feature \" that are generated from the input data. Increa ingly, however, re earch ha  focu ed on  tati tical model , which make  oft, probabili tic deci ion  ba ed on attaching real-valued weight  to each input feature (complex-valued embedding ,  and neural network  in general have al o been propo ed, for e.g.  peech ). Such model  have the advantage that they can expre  the relative certainty of many different po ible an wer  rather than only one, producing more reliable re ult  when  uch a model i  included a  a component of a larger  y tem.\n",
      "Some of the earlie t-u ed machine learning algorithm ,  uch a  deci ion tree , produced  y tem  of hard if-then rule   imilar to exi ting hand-written rule .  However, part-of- peech tagging introduced the u e of hidden Markov model  to natural language proce ing, and increa ingly, re earch ha  focu ed on  tati tical model , which make  oft, probabili tic deci ion  ba ed on attaching real-valued weight  to the feature  making up the input data. The cache language model  upon which many  peech recognition  y tem  now rely are example  of  uch  tati tical model .  Such model  are generally more robu t when given unfamiliar input, e pecially input that contain  error  (a  i  very common for real-world data), and produce more reliable re ult  when integrated into a larger  y tem compri ing multiple  ubta k .\n",
      "Since the neural turn,  tati tical method  in NLP re earch have been largely replaced by neural network . However, they continue to be relevant for context  in which  tati tical interpretability and tran parency i  required.\n",
      "A major drawback of  tati tical method  i  that they require elaborate feature engineering. Since 2015,  the field ha  thu  largely abandoned  tati tical method  and  hifted to neural network  for machine learning. Popular technique  include the u e of word embedding  to capture  emantic propertie  of word , and an increa e in end-to-end learning of a higher-level ta k (e.g., que tion an wering) in tead of relying on a pipeline of  eparate intermediate ta k  (e.g., part-of- peech tagging and dependency par ing). In  ome area , thi   hift ha  entailed  ub tantial change  in how NLP  y tem  are de igned,  uch that deep neural network-ba ed approache  may be viewed a  a new paradigm di tinct from  tati tical natural language proce ing. For in tance, the term neural machine tran lation (NMT) empha ize  the fact that deep learning-ba ed approache  to machine tran lation directly learn  equence-to- equence tran formation , obviating the need for intermediate  tep   uch a  word alignment and language modeling that wa  u ed in  tati tical machine tran lation (SMT). Late t work  tend to u e non-technical  tructure of a given ta k to build proper neural network. \n",
      "The following i  a li t of  ome of the mo t commonly re earched ta k  in natural language proce ing. Some of the e ta k  have direct real-world application , while other  more commonly  erve a   ubta k  that are u ed to aid in  olving larger ta k .\n",
      "Though natural language proce ing ta k  are clo ely intertwined, they can be  ubdivided into categorie  for convenience. A coar e divi ion i  given below.\n",
      "Ba ed on long- tanding trend  in the field, it i  po ible to extrapolate future direction  of NLP. A  of 2020, three trend  among the topic  of the long- tanding  erie  of CoNLL Shared Ta k  can be ob erved: \n",
      "Mo t higher-level NLP application  involve a pect  that emulate intelligent behaviour and apparent comprehen ion of natural language. More broadly  peaking, the technical operationalization of increa ingly advanced a pect  of cognitive behaviour repre ent  one of the developmental trajectorie  of NLP ( ee trend  among CoNLL  hared ta k  above).\n",
      "Cognition refer  to \"the mental action or proce  of acquiring knowledge and under tanding through thought, experience, and the  en e .\"  Cognitive  cience i  the interdi ciplinary,  cientific  tudy of the mind and it  proce e .  Cognitive lingui tic  i  an interdi ciplinary branch of lingui tic , combining knowledge and re earch from both p ychology and lingui tic .  E pecially during the age of  ymbolic NLP, the area of computational lingui tic  maintained  trong tie  with cognitive  tudie .\n",
      "A  an example, George Lakoff offer  a methodology to build natural language proce ing (NLP) algorithm  through the per pective of cognitive  cience, along with the finding  of cognitive lingui tic ,  with two defining a pect :\n",
      "Tie  with cognitive lingui tic  are part of the hi torical heritage of NLP, but they have been le  frequently addre ed  ince the  tati tical turn during the 1990 . Neverthele , approache  to develop cognitive model  toward  technically operationalizable framework  have been pur ued in the context of variou  framework , e.g., of cognitive grammar,  functional grammar,  con truction grammar,  computational p ycholingui tic  and cognitive neuro cience (e.g., ACT-R), however, with limited uptake in main tream NLP (a  mea ured by pre ence on major conference   of the ACL). More recently, idea  of cognitive NLP have been revived a  an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".  Likewi e, idea  of cognitive NLP are inherent to neural model  multimodal NLP (although rarely made explicit). \n",
      " Media related to Natural language proce ing at Wikimedia Common \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language proce ing  NLP  i  a  ubfield of lingui tic   computer  cience  and artificial intelligence concerned with the interaction  between computer  and human language  in particular how to program computer  to proce  and analyze large amount  of natural language data   The goal i  a computer capable of  under tanding  the content  of document   including the contextual nuance  of the language within them  The technology can then accurately extract information and in ight  contained in the document  a  well a  categorize and organize the document  them elve   Challenge  in natural language proce ing frequently involve  peech recognition  natural language under tanding  and natural language generation  Natural language proce ing ha  it  root  in the        Already in       Alan Turing publi hed an article titled  Computing Machinery and Intelligence  which propo ed what i  now called the Turing te t a  a criterion of intelligence  a ta k that involve  the automated interpretation and generation of natural language  but at the time not articulated a  a problem  eparate from artificial intelligence  The premi e of  ymbolic NLP i  well  ummarized by John Searle   Chine e room experiment  Given a collection of rule   e g   a Chine e phra ebook  with que tion  and matching an wer    the computer emulate  natural language under tanding  or other NLP ta k   by applying tho e rule  to the data it i  confronted with  Up to the        mo t natural language proce ing  y tem  were ba ed on complex  et  of hand written rule    Starting in the late        however  there wa  a revolution in natural language proce ing with the introduction of machine learning algorithm  for language proce ing   Thi  wa  due to both the  teady increa e in computational power   ee Moore   law  and the gradual le ening of the dominance of Chom kyan theorie  of lingui tic   e g  tran formational grammar   who e theoretical underpinning  di couraged the  ort of corpu  lingui tic  that underlie  the machine learning approach to language proce ing   In the        repre entation learning and deep neural network  tyle machine learning method  became wide pread in natural language proce ing  due in part to a flurry of re ult   howing that  uch technique    can achieve  tate of the art re ult  in many natural language ta k   for example in language modeling   par ing    and many other   Thi  i  increa ingly important in medicine and healthcare  where NLP i  being u ed to analyze note  and text in electronic health record  that would otherwi e be inacce ible for  tudy when  eeking to improve care   In the early day   many language proce ing  y tem  were de igned by  ymbolic method   i e   the hand coding of a  et of rule   coupled with a dictionary lookup     uch a  by writing grammar  or devi ing heuri tic rule  for  temming  More recent  y tem  ba ed on machine learning algorithm  have many advantage  over hand produced rule    De pite the popularity of machine learning in NLP re earch   ymbolic method  are  till        commonly u ed  Since the  o called   tati tical revolution    in the late       and mid        much natural language proce ing re earch ha  relied heavily on machine learning  The machine learning paradigm call  in tead for u ing  tati tical inference to automatically learn  uch rule  through the analy i  of large corpora  the plural form of corpu   i  a  et of document   po ibly with human or computer annotation   of typical real world example   Many different cla e  of machine learning algorithm  have been applied to natural language proce ing ta k   The e algorithm  take a  input a large  et of  feature   that are generated from the input data  Increa ingly  however  re earch ha  focu ed on  tati tical model   which make  oft  probabili tic deci ion  ba ed on attaching real valued weight  to each input feature  complex valued embedding    and neural network  in general have al o been propo ed  for e g   peech    Such model  have the advantage that they can expre  the relative certainty of many different po ible an wer  rather than only one  producing more reliable re ult  when  uch a model i  included a  a component of a larger  y tem  Some of the earlie t u ed machine learning algorithm    uch a  deci ion tree   produced  y tem  of hard if then rule   imilar to exi ting hand written rule    However  part of  peech tagging introduced the u e of hidden Markov model  to natural language proce ing  and increa ingly  re earch ha  focu ed on  tati tical model   which make  oft  probabili tic deci ion  ba ed on attaching real valued weight  to the feature  making up the input data  The cache language model  upon which many  peech recognition  y tem  now rely are example  of  uch  tati tical model    Such model  are generally more robu t when given unfamiliar input  e pecially input that contain  error   a  i  very common for real world data   and produce more reliable re ult  when integrated into a larger  y tem compri ing multiple  ubta k   Since the neural turn   tati tical method  in NLP re earch have been largely replaced by neural network   However  they continue to be relevant for context  in which  tati tical interpretability and tran parency i  required  A major drawback of  tati tical method  i  that they require elaborate feature engineering  Since        the field ha  thu  largely abandoned  tati tical method  and  hifted to neural network  for machine learning  Popular technique  include the u e of word embedding  to capture  emantic propertie  of word   and an increa e in end to end learning of a higher level ta k  e g   que tion an wering  in tead of relying on a pipeline of  eparate intermediate ta k   e g   part of  peech tagging and dependency par ing   In  ome area   thi   hift ha  entailed  ub tantial change  in how NLP  y tem  are de igned   uch that deep neural network ba ed approache  may be viewed a  a new paradigm di tinct from  tati tical natural language proce ing  For in tance  the term neural machine tran lation  NMT  empha ize  the fact that deep learning ba ed approache  to machine tran lation directly learn  equence to  equence tran formation   obviating the need for intermediate  tep   uch a  word alignment and language modeling that wa  u ed in  tati tical machine tran lation  SMT   Late t work  tend to u e non technical  tructure of a given ta k to build proper neural network   The following i  a li t of  ome of the mo t commonly re earched ta k  in natural language proce ing  Some of the e ta k  have direct real world application   while other  more commonly  erve a   ubta k  that are u ed to aid in  olving larger ta k   Though natural language proce ing ta k  are clo ely intertwined  they can be  ubdivided into categorie  for convenience  A coar e divi ion i  given below  Ba ed on long  tanding trend  in the field  it i  po ible to extrapolate future direction  of NLP  A  of       three trend  among the topic  of the long  tanding  erie  of CoNLL Shared Ta k  can be ob erved   Mo t higher level NLP application  involve a pect  that emulate intelligent behaviour and apparent comprehen ion of natural language  More broadly  peaking  the technical operationalization of increa ingly advanced a pect  of cognitive behaviour repre ent  one of the developmental trajectorie  of NLP   ee trend  among CoNLL  hared ta k  above   Cognition refer  to  the mental action or proce  of acquiring knowledge and under tanding through thought  experience  and the  en e     Cognitive  cience i  the interdi ciplinary   cientific  tudy of the mind and it  proce e    Cognitive lingui tic  i  an interdi ciplinary branch of lingui tic   combining knowledge and re earch from both p ychology and lingui tic    E pecially during the age of  ymbolic NLP  the area of computational lingui tic  maintained  trong tie  with cognitive  tudie   A  an example  George Lakoff offer  a methodology to build natural language proce ing  NLP  algorithm  through the per pective of cognitive  cience  along with the finding  of cognitive lingui tic    with two defining a pect   Tie  with cognitive lingui tic  are part of the hi torical heritage of NLP  but they have been le  frequently addre ed  ince the  tati tical turn during the        Neverthele   approache  to develop cognitive model  toward  technically operationalizable framework  have been pur ued in the context of variou  framework   e g   of cognitive grammar   functional grammar   con truction grammar   computational p ycholingui tic  and cognitive neuro cience  e g   ACT R   however  with limited uptake in main tream NLP  a  mea ured by pre ence on major conference   of the ACL   More recently  idea  of cognitive NLP have been revived a  an approach to achieve explainability  e g   under the notion of  cognitive AI    Likewi e  idea  of cognitive NLP are inherent to neural model  multimodal NLP  although rarely made explicit     Media related to Natural language proce ing at Wikimedia Common  \n"
     ]
    }
   ],
   "source": [
    "print(formatted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "words=word_tokenize(formatted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating word score\n",
    "word_frequencies={}\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        if w not in word_frequencies.keys():\n",
    "            word_frequencies[w]=1\n",
    "        else:\n",
    "            word_frequencies[w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi_freq=max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word]=(word_frequencies[word]/maxi_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Natural': 0.10344827586206896, 'language': 1.0, 'proce': 0.6896551724137931, 'ing': 0.7586206896551724, 'NLP': 0.5862068965517241, 'ubfield': 0.034482758620689655, 'lingui': 0.3103448275862069, 'tic': 0.4482758620689655, 'computer': 0.20689655172413793, 'cience': 0.13793103448275862, 'artificial': 0.06896551724137931, 'intelligence': 0.10344827586206896, 'concerned': 0.034482758620689655, 'interaction': 0.034482758620689655, 'human': 0.06896551724137931, 'particular': 0.034482758620689655, 'program': 0.034482758620689655, 'analyze': 0.06896551724137931, 'large': 0.10344827586206896, 'amount': 0.034482758620689655, 'natural': 0.6206896551724138, 'data': 0.1724137931034483, 'The': 0.2413793103448276, 'goal': 0.034482758620689655, 'capable': 0.034482758620689655, 'tanding': 0.20689655172413793, 'content': 0.034482758620689655, 'document': 0.13793103448275862, 'including': 0.034482758620689655, 'contextual': 0.034482758620689655, 'nuance': 0.034482758620689655, 'within': 0.034482758620689655, 'technology': 0.034482758620689655, 'accurately': 0.034482758620689655, 'extract': 0.034482758620689655, 'information': 0.034482758620689655, 'ight': 0.034482758620689655, 'contained': 0.034482758620689655, 'well': 0.06896551724137931, 'categorize': 0.034482758620689655, 'organize': 0.034482758620689655, 'elve': 0.034482758620689655, 'Challenge': 0.034482758620689655, 'frequently': 0.06896551724137931, 'involve': 0.10344827586206896, 'peech': 0.1724137931034483, 'recognition': 0.06896551724137931, 'generation': 0.06896551724137931, 'ha': 0.20689655172413793, 'root': 0.034482758620689655, 'Already': 0.034482758620689655, 'Alan': 0.034482758620689655, 'Turing': 0.06896551724137931, 'publi': 0.034482758620689655, 'hed': 0.034482758620689655, 'article': 0.034482758620689655, 'titled': 0.034482758620689655, 'Computing': 0.034482758620689655, 'Machinery': 0.034482758620689655, 'Intelligence': 0.034482758620689655, 'propo': 0.06896551724137931, 'ed': 0.5862068965517241, 'called': 0.06896551724137931, 'te': 0.034482758620689655, 'criterion': 0.034482758620689655, 'ta': 0.41379310344827586, 'k': 0.5172413793103449, 'automated': 0.034482758620689655, 'interpretation': 0.034482758620689655, 'time': 0.034482758620689655, 'articulated': 0.034482758620689655, 'problem': 0.034482758620689655, 'eparate': 0.06896551724137931, 'premi': 0.034482758620689655, 'e': 0.9655172413793104, 'ymbolic': 0.13793103448275862, 'ummarized': 0.034482758620689655, 'John': 0.034482758620689655, 'Searle': 0.034482758620689655, 'Chine': 0.06896551724137931, 'room': 0.034482758620689655, 'experiment': 0.034482758620689655, 'Given': 0.034482758620689655, 'collection': 0.034482758620689655, 'rule': 0.3103448275862069, 'g': 0.27586206896551724, 'phra': 0.034482758620689655, 'ebook': 0.034482758620689655, 'que': 0.06896551724137931, 'tion': 0.06896551724137931, 'matching': 0.034482758620689655, 'wer': 0.06896551724137931, 'emulate': 0.06896551724137931, 'applying': 0.034482758620689655, 'tho': 0.034482758620689655, 'confronted': 0.034482758620689655, 'Up': 0.034482758620689655, 'mo': 0.06896551724137931, 'tem': 0.27586206896551724, 'ba': 0.20689655172413793, 'complex': 0.06896551724137931, 'et': 0.13793103448275862, 'hand': 0.13793103448275862, 'written': 0.06896551724137931, 'Starting': 0.034482758620689655, 'late': 0.06896551724137931, 'however': 0.10344827586206896, 'wa': 0.10344827586206896, 'revolution': 0.06896551724137931, 'introduction': 0.034482758620689655, 'machine': 0.4482758620689655, 'learning': 0.4482758620689655, 'algorithm': 0.20689655172413793, 'Thi': 0.06896551724137931, 'due': 0.06896551724137931, 'teady': 0.034482758620689655, 'increa': 0.1724137931034483, 'computational': 0.10344827586206896, 'power': 0.034482758620689655, 'ee': 0.06896551724137931, 'Moore': 0.034482758620689655, 'law': 0.034482758620689655, 'gradual': 0.034482758620689655, 'le': 0.06896551724137931, 'ening': 0.034482758620689655, 'dominance': 0.034482758620689655, 'Chom': 0.034482758620689655, 'kyan': 0.034482758620689655, 'theorie': 0.034482758620689655, 'tran': 0.20689655172413793, 'formational': 0.034482758620689655, 'grammar': 0.1724137931034483, 'theoretical': 0.034482758620689655, 'underpinning': 0.034482758620689655, 'di': 0.06896551724137931, 'couraged': 0.034482758620689655, 'ort': 0.034482758620689655, 'corpu': 0.06896551724137931, 'underlie': 0.034482758620689655, 'approach': 0.06896551724137931, 'In': 0.10344827586206896, 'repre': 0.06896551724137931, 'entation': 0.034482758620689655, 'deep': 0.10344827586206896, 'neural': 0.3103448275862069, 'network': 0.20689655172413793, 'tyle': 0.034482758620689655, 'method': 0.20689655172413793, 'became': 0.034482758620689655, 'wide': 0.034482758620689655, 'pread': 0.034482758620689655, 'part': 0.13793103448275862, 'flurry': 0.034482758620689655, 'ult': 0.13793103448275862, 'howing': 0.034482758620689655, 'uch': 0.27586206896551724, 'technique': 0.06896551724137931, 'achieve': 0.06896551724137931, 'tate': 0.034482758620689655, 'art': 0.034482758620689655, 'many': 0.20689655172413793, 'example': 0.13793103448275862, 'modeling': 0.06896551724137931, 'par': 0.06896551724137931, 'ingly': 0.13793103448275862, 'important': 0.034482758620689655, 'medicine': 0.034482758620689655, 'healthcare': 0.034482758620689655, 'u': 0.3103448275862069, 'note': 0.034482758620689655, 'text': 0.034482758620689655, 'electronic': 0.034482758620689655, 'health': 0.034482758620689655, 'record': 0.034482758620689655, 'would': 0.034482758620689655, 'otherwi': 0.034482758620689655, 'inacce': 0.034482758620689655, 'ible': 0.10344827586206896, 'tudy': 0.06896551724137931, 'eeking': 0.034482758620689655, 'improve': 0.034482758620689655, 'care': 0.034482758620689655, 'early': 0.034482758620689655, 'day': 0.034482758620689655, 'de': 0.06896551724137931, 'igned': 0.06896551724137931, 'coding': 0.034482758620689655, 'coupled': 0.034482758620689655, 'dictionary': 0.034482758620689655, 'lookup': 0.034482758620689655, 'writing': 0.034482758620689655, 'devi': 0.034482758620689655, 'heuri': 0.034482758620689655, 'temming': 0.034482758620689655, 'More': 0.10344827586206896, 'recent': 0.034482758620689655, 'advantage': 0.06896551724137931, 'produced': 0.06896551724137931, 'De': 0.034482758620689655, 'pite': 0.034482758620689655, 'popularity': 0.034482758620689655, 'earch': 0.20689655172413793, 'till': 0.034482758620689655, 'commonly': 0.10344827586206896, 'Since': 0.10344827586206896, 'tati': 0.41379310344827586, 'tical': 0.41379310344827586, 'mid': 0.034482758620689655, 'much': 0.034482758620689655, 'relied': 0.034482758620689655, 'heavily': 0.034482758620689655, 'paradigm': 0.06896551724137931, 'call': 0.034482758620689655, 'tead': 0.06896551724137931, 'inference': 0.034482758620689655, 'automatically': 0.034482758620689655, 'learn': 0.06896551724137931, 'analy': 0.034482758620689655, 'corpora': 0.034482758620689655, 'plural': 0.034482758620689655, 'form': 0.034482758620689655, 'po': 0.10344827586206896, 'ibly': 0.034482758620689655, 'annotation': 0.034482758620689655, 'typical': 0.034482758620689655, 'real': 0.1724137931034483, 'world': 0.10344827586206896, 'Many': 0.034482758620689655, 'different': 0.06896551724137931, 'cla': 0.034482758620689655, 'applied': 0.034482758620689655, 'take': 0.034482758620689655, 'input': 0.20689655172413793, 'feature': 0.13793103448275862, 'generated': 0.034482758620689655, 'Increa': 0.034482758620689655, 'focu': 0.06896551724137931, 'model': 0.3448275862068966, 'make': 0.06896551724137931, 'oft': 0.06896551724137931, 'probabili': 0.06896551724137931, 'deci': 0.10344827586206896, 'ion': 0.1724137931034483, 'attaching': 0.06896551724137931, 'valued': 0.10344827586206896, 'weight': 0.06896551724137931, 'embedding': 0.06896551724137931, 'general': 0.034482758620689655, 'al': 0.034482758620689655, 'Such': 0.06896551724137931, 'expre': 0.034482758620689655, 'relative': 0.034482758620689655, 'certainty': 0.034482758620689655, 'rather': 0.034482758620689655, 'one': 0.06896551724137931, 'producing': 0.034482758620689655, 'reliable': 0.06896551724137931, 'included': 0.034482758620689655, 'component': 0.034482758620689655, 'larger': 0.10344827586206896, 'Some': 0.06896551724137931, 'earlie': 0.034482758620689655, 'tree': 0.034482758620689655, 'hard': 0.034482758620689655, 'imilar': 0.034482758620689655, 'exi': 0.034482758620689655, 'ting': 0.034482758620689655, 'However': 0.06896551724137931, 'tagging': 0.06896551724137931, 'introduced': 0.034482758620689655, 'hidden': 0.034482758620689655, 'Markov': 0.034482758620689655, 'making': 0.034482758620689655, 'cache': 0.034482758620689655, 'upon': 0.034482758620689655, 'rely': 0.034482758620689655, 'generally': 0.034482758620689655, 'robu': 0.034482758620689655, 'given': 0.10344827586206896, 'unfamiliar': 0.034482758620689655, 'pecially': 0.06896551724137931, 'contain': 0.034482758620689655, 'error': 0.034482758620689655, 'common': 0.034482758620689655, 'produce': 0.034482758620689655, 'integrated': 0.034482758620689655, 'compri': 0.034482758620689655, 'multiple': 0.034482758620689655, 'ubta': 0.06896551724137931, 'turn': 0.06896551724137931, 'largely': 0.06896551724137931, 'replaced': 0.034482758620689655, 'continue': 0.034482758620689655, 'relevant': 0.034482758620689655, 'context': 0.06896551724137931, 'interpretability': 0.034482758620689655, 'parency': 0.034482758620689655, 'required': 0.034482758620689655, 'A': 0.13793103448275862, 'major': 0.06896551724137931, 'drawback': 0.034482758620689655, 'require': 0.034482758620689655, 'elaborate': 0.034482758620689655, 'engineering': 0.034482758620689655, 'field': 0.06896551724137931, 'thu': 0.034482758620689655, 'abandoned': 0.034482758620689655, 'hifted': 0.034482758620689655, 'Popular': 0.034482758620689655, 'include': 0.034482758620689655, 'word': 0.10344827586206896, 'capture': 0.034482758620689655, 'emantic': 0.034482758620689655, 'propertie': 0.034482758620689655, 'end': 0.06896551724137931, 'higher': 0.06896551724137931, 'level': 0.06896551724137931, 'wering': 0.034482758620689655, 'relying': 0.034482758620689655, 'pipeline': 0.034482758620689655, 'intermediate': 0.06896551724137931, 'dependency': 0.034482758620689655, 'ome': 0.06896551724137931, 'area': 0.06896551724137931, 'thi': 0.034482758620689655, 'hift': 0.034482758620689655, 'entailed': 0.034482758620689655, 'ub': 0.034482758620689655, 'tantial': 0.034482758620689655, 'change': 0.034482758620689655, 'approache': 0.10344827586206896, 'may': 0.034482758620689655, 'viewed': 0.034482758620689655, 'new': 0.034482758620689655, 'tinct': 0.034482758620689655, 'For': 0.034482758620689655, 'tance': 0.034482758620689655, 'term': 0.034482758620689655, 'lation': 0.10344827586206896, 'NMT': 0.034482758620689655, 'empha': 0.034482758620689655, 'ize': 0.034482758620689655, 'fact': 0.034482758620689655, 'directly': 0.034482758620689655, 'equence': 0.06896551724137931, 'formation': 0.034482758620689655, 'obviating': 0.034482758620689655, 'need': 0.034482758620689655, 'tep': 0.034482758620689655, 'alignment': 0.034482758620689655, 'SMT': 0.034482758620689655, 'Late': 0.034482758620689655, 'work': 0.034482758620689655, 'tend': 0.034482758620689655, 'non': 0.034482758620689655, 'technical': 0.06896551724137931, 'tructure': 0.034482758620689655, 'build': 0.06896551724137931, 'proper': 0.034482758620689655, 'following': 0.034482758620689655, 'li': 0.034482758620689655, 'earched': 0.034482758620689655, 'direct': 0.034482758620689655, 'application': 0.06896551724137931, 'erve': 0.034482758620689655, 'aid': 0.034482758620689655, 'olving': 0.034482758620689655, 'Though': 0.034482758620689655, 'clo': 0.034482758620689655, 'ely': 0.034482758620689655, 'intertwined': 0.034482758620689655, 'ubdivided': 0.034482758620689655, 'categorie': 0.034482758620689655, 'convenience': 0.034482758620689655, 'coar': 0.034482758620689655, 'divi': 0.034482758620689655, 'Ba': 0.034482758620689655, 'long': 0.06896551724137931, 'trend': 0.10344827586206896, 'extrapolate': 0.034482758620689655, 'future': 0.034482758620689655, 'direction': 0.034482758620689655, 'three': 0.034482758620689655, 'among': 0.06896551724137931, 'topic': 0.034482758620689655, 'erie': 0.034482758620689655, 'CoNLL': 0.06896551724137931, 'Shared': 0.034482758620689655, 'Ta': 0.034482758620689655, 'ob': 0.034482758620689655, 'erved': 0.034482758620689655, 'Mo': 0.034482758620689655, 'pect': 0.10344827586206896, 'intelligent': 0.034482758620689655, 'behaviour': 0.06896551724137931, 'apparent': 0.034482758620689655, 'comprehen': 0.034482758620689655, 'broadly': 0.034482758620689655, 'peaking': 0.034482758620689655, 'operationalization': 0.034482758620689655, 'advanced': 0.034482758620689655, 'cognitive': 0.3793103448275862, 'ent': 0.034482758620689655, 'developmental': 0.034482758620689655, 'trajectorie': 0.034482758620689655, 'hared': 0.034482758620689655, 'Cognition': 0.034482758620689655, 'refer': 0.034482758620689655, 'mental': 0.034482758620689655, 'action': 0.034482758620689655, 'acquiring': 0.034482758620689655, 'knowledge': 0.06896551724137931, 'thought': 0.034482758620689655, 'experience': 0.034482758620689655, 'en': 0.034482758620689655, 'Cognitive': 0.06896551724137931, 'interdi': 0.06896551724137931, 'ciplinary': 0.06896551724137931, 'cientific': 0.034482758620689655, 'mind': 0.034482758620689655, 'branch': 0.034482758620689655, 'combining': 0.034482758620689655, 'p': 0.06896551724137931, 'ychology': 0.034482758620689655, 'E': 0.034482758620689655, 'age': 0.034482758620689655, 'maintained': 0.034482758620689655, 'trong': 0.034482758620689655, 'tie': 0.034482758620689655, 'tudie': 0.034482758620689655, 'George': 0.034482758620689655, 'Lakoff': 0.034482758620689655, 'offer': 0.034482758620689655, 'methodology': 0.034482758620689655, 'per': 0.034482758620689655, 'pective': 0.034482758620689655, 'along': 0.034482758620689655, 'finding': 0.034482758620689655, 'two': 0.034482758620689655, 'defining': 0.034482758620689655, 'Tie': 0.034482758620689655, 'hi': 0.034482758620689655, 'torical': 0.034482758620689655, 'heritage': 0.034482758620689655, 'addre': 0.034482758620689655, 'ince': 0.034482758620689655, 'Neverthele': 0.034482758620689655, 'develop': 0.034482758620689655, 'toward': 0.034482758620689655, 'technically': 0.034482758620689655, 'operationalizable': 0.034482758620689655, 'framework': 0.06896551724137931, 'pur': 0.034482758620689655, 'ued': 0.034482758620689655, 'variou': 0.034482758620689655, 'functional': 0.034482758620689655, 'con': 0.034482758620689655, 'truction': 0.034482758620689655, 'ycholingui': 0.034482758620689655, 'neuro': 0.034482758620689655, 'ACT': 0.034482758620689655, 'R': 0.034482758620689655, 'limited': 0.034482758620689655, 'uptake': 0.034482758620689655, 'main': 0.034482758620689655, 'tream': 0.034482758620689655, 'mea': 0.034482758620689655, 'ured': 0.034482758620689655, 'pre': 0.034482758620689655, 'ence': 0.034482758620689655, 'conference': 0.034482758620689655, 'ACL': 0.034482758620689655, 'recently': 0.034482758620689655, 'idea': 0.06896551724137931, 'revived': 0.034482758620689655, 'explainability': 0.034482758620689655, 'notion': 0.034482758620689655, 'AI': 0.034482758620689655, 'Likewi': 0.034482758620689655, 'inherent': 0.034482758620689655, 'multimodal': 0.034482758620689655, 'although': 0.034482758620689655, 'rarely': 0.034482758620689655, 'made': 0.034482758620689655, 'explicit': 0.034482758620689655, 'Media': 0.034482758620689655, 'related': 0.034482758620689655, 'Wikimedia': 0.034482758620689655, 'Common': 0.034482758620689655}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies,sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating sentence score\n",
    "sentence_scores={}\n",
    "for sent in sentences:\n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if(len(sent.split(' '))< 30):\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent]=word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The goal i  a computer capable of \"under tanding\" the content  of document , including the contextual nuance  of the language within them.': 1.7931034482758619, 'Challenge  in natural language proce ing frequently involve  peech recognition, natural language under tanding, and natural language generation.': 7.0, 'Natural language proce ing ha  it  root  in the 1950 .': 3.3103448275862064, 'Up to the 1980 , mo t natural language proce ing  y tem  were ba ed on complex  et  of hand-written rule .': 4.724137931034483, 'Starting in the late 1980 , however, there wa  a revolution in natural language proce ing with the introduction of machine learning algorithm  for language proce ing.': 7.0, 'tran formational grammar), who e theoretical underpinning  di couraged the  ort of corpu  lingui tic  that underlie  the machine-learning approach to language proce ing.': 4.9655172413793105, 'Many different cla e  of machine-learning algorithm  have been applied to natural-language-proce ing ta k .': 3.206896551724138, 'The e algorithm  take a  input a large  et of \"feature \" that are generated from the input data.': 2.2068965517241383, 'peech ).': 0.1724137931034483, 'The cache language model  upon which many  peech recognition  y tem  now rely are example  of  uch  tati tical model .': 3.7586206896551717, 'Since the neural turn,  tati tical method  in NLP re earch have been largely replaced by neural network .': 2.2413793103448274, 'However, they continue to be relevant for context  in which  tati tical interpretability and tran parency i  required.': 1.3793103448275863, 'A major drawback of  tati tical method  i  that they require elaborate feature engineering.': 1.3793103448275865, 'Since 2015,  the field ha  thu  largely abandoned  tati tical method  and  hifted to neural network  for machine learning.': 2.8965517241379306, 'Late t work  tend to u e non-technical  tructure of a given ta k to build proper neural network.': 3.103448275862069, 'The following i  a li t of  ome of the mo t commonly re earched ta k  in natural language proce ing.': 4.344827586206897, 'Though natural language proce ing ta k  are clo ely intertwined, they can be  ubdivided into categorie  for convenience.': 4.206896551724137, 'A coar e divi ion i  given below.': 1.3103448275862069, 'Ba ed on long- tanding trend  in the field, it i  po ible to extrapolate future direction  of NLP.': 1.4827586206896552, 'Cognition refer  to \"the mental action or proce  of acquiring knowledge and under tanding through thought, experience, and the  en e .\"': 2.1724137931034484, 'Cognitive  cience i  the interdi ciplinary,  cientific  tudy of the mind and it  proce e .': 2.4482758620689657, 'Cognitive lingui tic  i  an interdi ciplinary branch of lingui tic , combining knowledge and re earch from both p ychology and lingui tic .': 3.2413793103448274, 'E pecially during the age of  ymbolic NLP, the area of computational lingui tic  maintained  trong tie  with cognitive  tudie .': 2.655172413793103, 'More recently, idea  of cognitive NLP have been revived a  an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".': 1.103448275862069, 'Likewi e, idea  of cognitive NLP are inherent to neural model  multimodal NLP (although rarely made explicit).': 2.2758620689655165, 'Media related to Natural language proce ing at Wikimedia Common': 3.137931034482759}\n"
     ]
    }
   ],
   "source": [
    "print(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "summary_sentences=heapq.nlargest(7,sentence_scores,key=sentence_scores.get)\n",
    "summary=''.join(summary_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge  in natural language proce ing frequently involve  peech recognition, natural language under tanding, and natural language generation.Starting in the late 1980 , however, there wa  a revolution in natural language proce ing with the introduction of machine learning algorithm  for language proce ing.tran formational grammar), who e theoretical underpinning  di couraged the  ort of corpu  lingui tic  that underlie  the machine-learning approach to language proce ing.Up to the 1980 , mo t natural language proce ing  y tem  were ba ed on complex  et  of hand-written rule .The following i  a li t of  ome of the mo t commonly re earched ta k  in natural language proce ing.Though natural language proce ing ta k  are clo ely intertwined, they can be  ubdivided into categorie  for convenience.The cache language model  upon which many  peech recognition  y tem  now rely are example  of  uch  tati tical model .\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
